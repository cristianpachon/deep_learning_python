{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to functional API\n",
    "\n",
    "Let's start with a minimal example that shows side by side a simple Sequential model and its\n",
    "equivalent in the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation = 'relu', input_shape = (64, )))\n",
    "seq_model.add(layers.Dense(32, activation = 'relu'))\n",
    "seq_model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape = (64, ))\n",
    "x = layers.Dense(32, activation = 'relu')(input_tensor)\n",
    "x = layers.Dense(32, activation = 'relu')(x)\n",
    "output_tensor = layers.Dense(10, activation = 'softmax')(x)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000,64))\n",
    "y_train = np.random.random((1000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 892us/step - loss: 14.3355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f85be6904d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 66us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.862804656982423"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03192253, 0.03139446, 0.53079146, 0.00996405, 0.00749125,\n",
       "        0.27539852, 0.00253185, 0.00187797, 0.100104  , 0.00852388],\n",
       "       [0.02828155, 0.04114874, 0.5928531 , 0.01042755, 0.00764882,\n",
       "        0.22347376, 0.00261179, 0.00236382, 0.07790326, 0.01328758]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_train)\n",
    "predictions[:2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input models\n",
    "Following is an example of how you can build such a model with the functional API. You set up two independent brances, encoding the tect input and the question input as representation vectors; then, concatenate these vectors; and finally, add a softmax classifier on top of the concatenated representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API implementation of a two-input question-answering model\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape = (None, ), dtype = 'int32', name = 'text')\n",
    "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
    "encoded_text = layers.LSTM(32)(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_input = Input(shape = (None, ), dtype = 'int32', name = 'question')\n",
    "embedded_question = layers.Embedding(64, question_vocabulary_size)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = layers.Dense(answer_vocabulary_size, activation = 'softmax')(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you train this two-input model? There are two possible APIs: you can feed the model a list of Numpy arrays as inputs, or you can feed it a dictionary that maps unput names to Numpy arrays. Naturally, the latter option is available only if you give names to your inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size, size = (num_samples, max_length))\n",
    "question = np.random.randint(1, answer_vocabulary_size, size = (num_samples, max_length))\n",
    "answers = np.random.randint(0, 2, size = (num_samples, answer_vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1571.2929 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1571.7502 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1572.1397 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1572.4710 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1572.7944 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.0321 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.2440 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.4420 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.6016 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.7155 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f85bc0bec10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([text, question], answers, epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.8995 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1573.9765 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.0492 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.1225 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.1997 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.2664 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.3078 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.3482 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.4124 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1574.4209 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f85bc13a0d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit({'text': text, 'question' : question}, answers, epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "num_income_groups = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_input = Input(shape = (None, ), dtype = 'int32', name = 'posts')\n",
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation = 'relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "#Output. We are naming them\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups, activation = 'softmax', name = 'income')(x)\n",
    "gender_prediction = layers.Dense(1, activation = 'sigmoid', name = 'gender')(x)\n",
    "\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, training such a model requires the ability to specify different loss function for different heads of th network: for instance, age prediction is a scalar regression task, but gender prediction is a binary classification task, requiring a different training procedure.\n",
    "But because gradient descent requires you to minimize a scalar, you must combine different losses into a single value in oorder to train the model. The simplest way to combine different losses is to sum them all.\n",
    "In keras, you can use either a list or a dictionary of losses in compile t ospecify different objects for different outputs; the resulting loss values are summed into a global loss, which is minimized during traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilation options of multi-output model: multiple losses\n",
    "model.compile(optimizer = 'rmsprop', loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "#model.compile(optimizer = 'rmsprop', \n",
    "#              loss = {\n",
    "#                  'age':'mse', \n",
    "#                  'income':'categorical_crossentropy', \n",
    "#                  'gender':'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that very imbalanced loss contributions will cause the model representation to be optimized preferentially for the task with the largest individual loss, at the expense of the other tasks.\n",
    "To remedy this, you can assign different levels of importance to the loss values in their contribution to the final loss. This is useful in particular if the losses' values use different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights = [0.25,1.,10.])\n",
    "\n",
    "#model.compile(optimizer = 'rmsprop', \n",
    "#              loss = {\n",
    "#                  'age':'mse', \n",
    "#                  'income':'categorical_crossentropy', \n",
    "#                  'gender':'binary_crossentropy'},\n",
    "#             loss_weights = {'age':0.25,\n",
    "#                             'income':1.,\n",
    "#                             'gender':10.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data is assumed to be numpy arrays\n",
    "#model.fit(posts, [age_targets, income_targets, gender_targets], epochs = 10, batch_size = 64)\n",
    "#model.fit(posts, {'age':age_targets, 'income':income_targets, 'gender':gender_targets}, epochs = 10, batch_size = 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
