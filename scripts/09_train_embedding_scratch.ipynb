{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dir = '/home/jupyter/deep_learning_python/data/imdb/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            \n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[\"It's difficult to put into words the almost seething hatred I have of this film. But I'll try:<br /><br />Every other word was an expletive, the sex scenes were uncomfortable, drugs were rampant and stereotyping was beyond the norm, if not offensive to Italian-Americans.<br /><br />I'm not saying the acting was terrible, because Leguizamo, Sorvino, Brody, Espisito et. al, performed well. But...almost every character in the film I despised. Not since The Bonfire of the Vanities have I disliked every character on screen.\", \"My teacher taped this and showed it to us in Child Care to demonstrate how teen pregnancy affects people. It just demonstrated how teen pregnancy affects a childish jock not properly educated on how sex works and a whiny, unloved girl who throws fruit when angry and couldn't tell she was with the wrong man even if he wore a sign stating he was such. I wouldn't be surprised if the father of the baby had about eight girlfriends in the first edition of the script. Stacy's (the carrier of the baby) mother is a riot. She is oblivious to the fact her daughter is past the age of four and is seemingly unshaken when people spy on her through her dining room window. Bobby's (the father) best friend's name is Dewey, and is an obvious rip off of Sean Penn's character in Fast Times at Ridgemont High. This movie is horrid, simply because none of the characters are believable. Thank goodness it's only made for TV, limiting the public's chances of viewing it.\"]\n"
     ]
    }
   ],
   "source": [
    "print(labels[0:10])\n",
    "print(texts[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts off reviews after 100 words\n",
    "maxlen = 100\n",
    "\n",
    "# Trains on 200 samples\n",
    "training_samples = 200\n",
    "\n",
    "# Validate on 10000 samples\n",
    "validation_samples = 10000\n",
    "\n",
    "# Consider only the top 10000 words in the dataset\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences is a list of lists\n",
    "# sequences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88584 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (25000, 100))\n",
      "('Shape of label tensor:', (25000,))\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into a training set and a validation set, but first shuffles the data,\n",
    "# since we are starting with data in ehich samples are ordered (all negative first)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the GloVe word-embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "globe_dir = '/home/jupyter/deep_learning_python/data/pretrained_models/embeddings/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(globe_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the GloVe word-embedding index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the GloVe embedding in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0492 - acc: 1.0000 - val_loss: 3.1662e-05 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0799e-07 - acc: 1.0000 - val_loss: 3.1662e-05 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0814e-07 - acc: 1.0000 - val_loss: 3.1661e-05 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0829e-07 - acc: 1.0000 - val_loss: 3.1661e-05 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0814e-07 - acc: 1.0000 - val_loss: 3.1660e-05 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0829e-07 - acc: 1.0000 - val_loss: 3.1659e-05 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0799e-07 - acc: 1.0000 - val_loss: 3.1658e-05 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0799e-07 - acc: 1.0000 - val_loss: 3.1656e-05 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0809e-07 - acc: 1.0000 - val_loss: 3.1650e-05 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0824e-07 - acc: 1.0000 - val_loss: 3.1646e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_data = (x_val, y_val)\n",
    ")\n",
    "model.save_weights('/home/jupyter/deep_learning_python/models/pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the same model without pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.6936 - acc: 0.4850 - val_loss: 0.6937 - val_acc: 0.5259\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5024 - acc: 0.9800 - val_loss: 0.7034 - val_acc: 0.5293\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2934 - acc: 0.9850 - val_loss: 0.6977 - val_acc: 0.5185\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1352 - acc: 1.0000 - val_loss: 0.7053 - val_acc: 0.5369\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0653 - acc: 1.0000 - val_loss: 0.7215 - val_acc: 0.5338\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.5369\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.7346 - val_acc: 0.5412\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.7361 - val_acc: 0.5386\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.7426 - val_acc: 0.5400\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7534 - val_acc: 0.5342\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['acc']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_data = (x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(\n",
    "    imdb_dir,\n",
    "    'test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            \n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 51us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.000380046177654, 0.5]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('/home/jupyter/deep_learning_python/models/pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
